# Libraries
import itertools
import numpy as np
import torch
import torch.optim as optim
# Local
from agents.ddpg import DDPG

class TD3(DDPG):
    """
        Algorithm:      TD3 (Twin Delayed DDPG)
        Paper:          https://arxiv.org/abs/1802.09477
        Paper authors:  Scott Fujimoto [1], Herke van Hoof [2],
                        and David Meger [1]
        Institution:    [1] McGill University, [2] University of Amsterdam

        TD3 implements three changes to DDPG:

        Clipped Double-Q Learning
            TD3 learns two Q-functions instead of one (hence “twin”), and uses
            the smaller of the two Q-values to form the targets in the Bellman
            error loss functions.

        "Delayed" Policy Updates
            TD3 updates the policy (and target networks) less frequently than
            the Q-function. The paper recommends one policy update for every two
            Q-function updates.

        Target Policy Smoothing
            TD3 adds noise to the target action, to make it harder for the
            policy to exploit Q-function errors by smoothing out Q along
            changes in action.
    """
    def __init__(self, env, model, buffer, logger, args):
        super().__init__(env, model, buffer, logger, args)
        """ Two Q-networks """
        self.q_params = itertools.chain(self.model.q1.parameters(), self.model.q2.parameters())
        self.q_optimizer = optim.Adam(self.q_params, lr=args.q_lr)

    """
        Functions to be overloaded from the parent class.
    """

    def _get_action(self, o):
        # Get actions just like DDPG
        return DDPG._get_action(self, o)

    def _update_networks(self):
        # Update `train_q_iters` times
        for self.train_q_iter in range(self.args.train_q_iters):
            # Load the transitions from the buffer
            batch = self.buffer.sample_batch(self.args.batch_size)
            # First run one gradient descent step for Q1 and Q2
            self._update_q(batch)

            """ Delayed Policy Updates """
            if self.train_q_iter % self.args.policy_delay == 0:
                # Next run one gradient descent step for pi.
                self._update_policy(batch)
                # Finally, update target networks
                self._update_target()

    """
        Algorithm-specific functions.
    """

    ################################ Losses ################################

    def _compute_pi_loss(self, o):
        """
            Compute the loss for the policy.
        """
        pi = self.model.pi(o)
        # Always get the policy loss from Q1
        q1_pi = self.model.q1(o, pi)
        return -q1_pi.mean()

    def _get_q_values(self, o, a):
        """
            Get action-value estimates (Q-values) from networks.
        """
        # Get Q-values from Q1 and Q2; return as a dictionary
        return {"q1": self.model.q1(o,a), "q2": self.model.q2(o,a)}

    ###################### Mean Squared Bellman Error ######################

    def _get_Bellman_targets(self, o2, pi_targ):
        """
            Compute targets for the Bellman equation.
        """
        # Target policy smoothing
        a2 = self._target_policy_smoothing(pi_targ)
        # Use the smaller of the two Q-values to form the targets in the
        # Bellman error
        q_pi_targ = self._clipped_double_q_learning(self.target, o2, a2)
        return q_pi_targ

    def _target_policy_smoothing(self, pi_targ):
        """
            Target policy smoothing
                Adds some noise to the target action, which makes it difficult
                for the policy to generate the exact `a` that would exploit the
                Q-network and generate a wild action-value estimate.
        """
        epsilon = torch.randn_like(pi_targ) * self.args.target_noise
        epsilon = torch.clamp(epsilon, -self.args.noise_clip, self.args.noise_clip)
        a2 = pi_targ + epsilon
        a2 = torch.clamp(a2, -self.args.act_limit, self.args.act_limit)
        return a2

    def _clipped_double_q_learning(self, model, o, a):
        """
            Clipped Double-Q Learning
                Returns the minimum of the Q-values generated by two
                Q-networks, preventing overestimation of action-values.
        """
        q1_pi = model.q1(o, a)
        q2_pi = model.q2(o, a)
        # Return the smaller of the two Q-values
        q_pi = torch.min(q1_pi, q2_pi)
        return q_pi

    def _compute_MSE(self, qs, backup):
        """
            Compute mean squared error between the Q-values from the network and
            the computed Bellman backup.
        """
        # Loop through the dictionary of Q-values, adding the MSE to a list,
        # then sum the list.
        losses_q = [((q - backup)**2).mean() for _, q in qs.items()]
        loss_q = sum(losses_q)
        return loss_q
